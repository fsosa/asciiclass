Reflection - Fidel Sosa
========

## ProtoBuf Results

NOTE: All code for the protobuf examples can be found at my Github: https://github.com/fsosa/asciiclass/blob/master/labs/lab2/proto_analyzer.py

1. Find the number of deleted messages in the dataset.

    Solution: Loop through all the tweets and keep a count of deleted tweets by checking the is_delete attribute
       Number of deleted tweets: 1554

2. Find the number of tweets that are replies to another tweet in this dataset.
	
    Solution: Create a list of all unique tweet ids, then loop through all the tweets and check if reply_to attribute is in the list of ids
	    Number of tweets in reply: 17

3. Find the five uids that have tweeted the most.
	
    Solution: Create a dictionary of user ids to tweet counts, ignoring deleted tweets. Then sort the keys based on tweet counts and take the top five. 
	    Top five tweeters: [1269521828L, 392695315L, 424808364L, 1706901902L, 1471774728L]

3. Find the names of the top five places by number of tweets. (Tweets may have a "place" attribute that describes where the tweet is from).
	
    Solution: Create a dictionary of place names to tweet occurences. Then sort the keys based on the tweet occurences and take the top five. 
	    Top five places: [u'T\xfcrkiye', u'Gambir', u'Mississippi', u'Malalayang', u'Nongsa']



## SQL Results

1. Find the number of deleted messages in the dataset.
        sqlite> SELECT COUNT(*) FROM tweets WHERE is_delete=1
	    1554

2. Find the number of tweets that are replies to another tweet in this dataset.
        sqlite> SELECT COUNT(*) FROM tweets INNER JOIN tweets as b WHERE tweets.reply_to = b.id;
	    17

3. Find the five uids that have tweeted the most.

        sqlite> SELECT uid, COUNT(uid) FROM tweets WHERE tweets.is_delete = 0 GROUP BY uid ORDER BY COUNT(uid) desc LIMIT 5;
	    1269521828|5
	    392695315|4
	    424808364|3
	    1706901902|3
	    23991910|2

3. Find the names of the top five places by number of tweets. (Tweets may have a "place" attribute that describes where the tweet is from).

        sqlite> SELECT places.name, COUNT(places.tid) FROM places, tweets WHERE places.tid = tweets.id GROUP BY places.name ORDER BY COUNT(places.tid) DESC LIMIT 5;
    	Türkiye|4
    	Gambir|3
    	Bandar Seremban|2
    	East Borneo|2
    	Florida|2
	

## Mongo Results

1. Find the number of deleted messages in the dataset.
	    > db.tweets.count({delete: {$exists: true}})
	    1554

2. Find the number of tweets that are replies to another tweet in this dataset.
	    > db.tweets.count({in_reply_to_status_id: {$in : db.tweets.distinct('id')}})
	    17


3. Find the five uids that have tweeted the most.
    	> db.tweets.aggregate( {$match: {delete: {$exists: false}}}, {$group: {_id: '$user.id', total: {$sum: 1}}}, {$sort: {total:-1}}, {$limit: 5} )
    	{
    	 "result" : [
    	  {
    	   "_id" : 1269521828,
    	   "total" : 5
    	  },
    	  {
    	   "_id" : 392695315,
    	   "total" : 4
    	  },
    	  {
    	   "_id" : 1706901902,
    	   "total" : 3
    	  },
    	  {
    	   "_id" : 424808364,
    	   "total" : 3
    	  },
    	  {
    	   "_id" : 1607263058,
    	   "total" : 2
    	  }
    	 ],
    	 "ok" : 1
    	}



3. Find the names of the top five places by number of tweets. (Tweets may have a "place" attribute that describes where the tweet is from).
    	> db.tweets.aggregate( {$match: {'place.name': {$exists: true} }}, {$group: {_id: '$place.name', total: {$sum: 1}}}, {$sort: {total:-1}}, {$limit: 5})
    	{
    	 "result" : [
    	  {
    	   "_id" : "Türkiye",
    	   "total" : 4
    	  },
    	  {
    	   "_id" : "Gambir",
    	   "total" : 3
    	  },
    	  {
    	   "_id" : "East Borneo",
    	   "total" : 2
    	  },
    	  {
    	   "_id" : "Malalayang",
    	   "total" : 2
    	  },
    	  {
    	   "_id" : "Mississippi",
    	   "total" : 2
    	  }
    	 ],
    	 "ok" : 1
    	}
		
## Responses

1. Read the schema and protocol buffer definition files. What are the main differences between the two? Are there any similarities?
	
    The protocol buffer definition files specify an order (the tag value) while the database schema enforces no such order. The protobuf definition also enforces whether or not a field is required, whereas the schema does not. Finally, you can compose protocol buffer messages with each other, which you can't do in the db schema directly. This is achieved in the schema through the use of foreign keys. However, both definitions force you to assign types to fields. 
	
2. Describe one question that would be easier to answer with protocol buffers than via a SQL query.
	
    Protocol buffers allow you to easily serialize/deserialize data and perform any kind of operation on the data structures. If a task required a lot of custom logic and processing, it would be easier to 
	use protocol buffers than SQL because of the ease of writing the code yourself in any language with a protocol buffer library. Otherwise, you would be limited to writing the custom logic in SQL, which might be overly complex for the task.  
	
3. Describe one question that would be easier to answer with MongoDB than via a SQL query.
	
    It would be easier to perform a query on a collection of possibly disparate data in Mongo than in SQL. For example, in this lab a schema had to be pre-defined to be able to work with the twitter data in SQL. Mongo was able to import the JSON data directly. The Mongo collection contained disparate JSON objects (e.g. deleted vs actual tweets) and it was simple enough to write queries without worrying about the differences. 
	
4. Describe one question that would be easier to answer via a SQL query than using MongoDB.
	
    Performing joins and aggregates on sets of data is much simpler in SQL than in Mongo. The Mongo query language is significantly less self-describing and more difficult to compose than SQL. Furthermore, the lack of consistency between objects in the collection might necessitate stricter queries as one has to account for disparity between object definitions. 
	
5. What fields in the original JSON structure would be difficult to convert to relational database schemas?
	
    Fields with multiple levels of hierarchy, such as place, are difficult to convert to a relational database schema because of the necessity for multiple tables to describe all levels of hiearchy. For example, a place has a bounding box which consists of a coordinate list and type, which might necessitate separate tables. 
	
6. In terms of lines of code, when did various approaches shine? Think about the challenges of defining schemas, loading and storing the data, and running queries.
	
    The SQL approach was the simplest in terms of getting the results. The queries were the simplest to write and short. The schema was relatively short as well. The protocol buffer approach took the longest to write due to the need to	write all the custom query lookup code, but was the most flexible due to this very reason. The Mongo approach was the simplest in loading the data into the database, but the query language was more complex than SQL and aggregates were not simple to write. 

7. What other metrics (e.g., time to implement, code redundancy, etc.) can we use to compare these different approaches? Which system is better by those measures?
	
    In terms of time to implement SQL was the shortest, followed by Mongo and then protocol buffers. The need to write our own query logic took way more time than looking up documentation on the existing query languages. The protocol buffer approach was the most redudant for this reason as well. It would take more time to write a modular query system than simply loading the data in SQL or Mongo and performing the queries. Data cleanup might also be a metric to consider. Mongo wins in this regard by being able to simply load up a huge JSON object; SQL and protobufs require a schema or definition file and a way of converting and importing the data into a compatible format. 
	
	
8. How long did this lab take you? We want to make sure to target future labs to not take too much of your time.
	
    The lab took me about 7 hours total. 